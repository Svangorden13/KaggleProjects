{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16bff921",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-09-05T22:50:53.964090Z",
     "iopub.status.busy": "2023-09-05T22:50:53.963553Z",
     "iopub.status.idle": "2023-09-05T22:50:53.982307Z",
     "shell.execute_reply": "2023-09-05T22:50:53.981042Z"
    },
    "papermill": {
     "duration": 0.035131,
     "end_time": "2023-09-05T22:50:53.984832",
     "exception": false,
     "start_time": "2023-09-05T22:50:53.949701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/spaceship-titanic/sample_submission.csv\n",
      "/kaggle/input/spaceship-titanic/train.csv\n",
      "/kaggle/input/spaceship-titanic/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ef75d61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:50:54.003776Z",
     "iopub.status.busy": "2023-09-05T22:50:54.003296Z",
     "iopub.status.idle": "2023-09-05T22:51:05.693932Z",
     "shell.execute_reply": "2023-09-05T22:51:05.692442Z"
    },
    "papermill": {
     "duration": 11.703588,
     "end_time": "2023-09-05T22:51:05.697038",
     "exception": false,
     "start_time": "2023-09-05T22:50:53.993450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18dc090b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:51:05.719032Z",
     "iopub.status.busy": "2023-09-05T22:51:05.717789Z",
     "iopub.status.idle": "2023-09-05T22:51:05.875636Z",
     "shell.execute_reply": "2023-09-05T22:51:05.874328Z"
    },
    "papermill": {
     "duration": 0.171817,
     "end_time": "2023-09-05T22:51:05.878262",
     "exception": false,
     "start_time": "2023-09-05T22:51:05.706445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train dataset shape is (8693, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HomePlanet      0\n",
       "CryoSleep       0\n",
       "Destination     0\n",
       "Age             0\n",
       "VIP             0\n",
       "RoomService     0\n",
       "FoodCourt       0\n",
       "ShoppingMall    0\n",
       "Spa             0\n",
       "VRDeck          0\n",
       "Transported     0\n",
       "Deck            0\n",
       "Cabin_num       0\n",
       "Side            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a dataset into a Pandas Dataframe\n",
    "dataset_df = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\n",
    "print(\"Full train dataset shape is {}\".format(dataset_df.shape))\n",
    "\n",
    "dataset_df = dataset_df.drop(['PassengerId', 'Name'], axis=1)\n",
    "dataset_df[[\"Deck\", \"Cabin_num\", \"Side\"]] = dataset_df[\"Cabin\"].str.split(\"/\", expand=True)\n",
    "dataset_df = dataset_df.drop('Cabin', axis=1)\n",
    "dataset_df.fillna(0, inplace=True)\n",
    "dataset_df['Transported'] = dataset_df['Transported'].astype(int)\n",
    "dataset_df['VIP'] = dataset_df['VIP'].astype(int)\n",
    "dataset_df['CryoSleep'] = dataset_df['CryoSleep'].astype(int)\n",
    "dataset_df.head(5)\n",
    "dataset_df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f16e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:51:05.898842Z",
     "iopub.status.busy": "2023-09-05T22:51:05.898432Z",
     "iopub.status.idle": "2023-09-05T22:51:05.910809Z",
     "shell.execute_reply": "2023-09-05T22:51:05.909302Z"
    },
    "papermill": {
     "duration": 0.025535,
     "end_time": "2023-09-05T22:51:05.913431",
     "exception": false,
     "start_time": "2023-09-05T22:51:05.887896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6941 examples in training, 1752 examples in testing.\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(dataset, test_ratio=0.20):\n",
    "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
    "  return dataset[~test_indices], dataset[test_indices]\n",
    "\n",
    "train_ds_pd, valid_ds_pd = split_dataset(dataset_df)\n",
    "print(\"{} examples in training, {} examples in testing.\".format(\n",
    "    len(train_ds_pd), len(valid_ds_pd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efcaf9f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:51:05.933960Z",
     "iopub.status.busy": "2023-09-05T22:51:05.932826Z",
     "iopub.status.idle": "2023-09-05T22:51:05.941568Z",
     "shell.execute_reply": "2023-09-05T22:51:05.940688Z"
    },
    "papermill": {
     "duration": 0.021527,
     "end_time": "2023-09-05T22:51:05.944113",
     "exception": false,
     "start_time": "2023-09-05T22:51:05.922586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_columns(dataframe, target=None, features=None):\n",
    "    for name in dataframe.columns.values:\n",
    "        if name != target:\n",
    "            if is_numeric_dtype(dataframe[name]):\n",
    "                dataframe[name] = dataframe[name].astype('float')\n",
    "                if features != None:\n",
    "                    feature_columns.append(tf.feature_column.numeric_column(name))\n",
    "            else:\n",
    "                dataframe[name] = dataframe[name].astype('string')\n",
    "                dataframe[name] = dataframe[name].astype('category')\n",
    "                if features != None:\n",
    "                    features.append(tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_vocabulary_list(name, vocabulary_list=dataframe[name].unique()), dimension=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0fc036c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:51:05.965089Z",
     "iopub.status.busy": "2023-09-05T22:51:05.964455Z",
     "iopub.status.idle": "2023-09-05T22:51:06.022632Z",
     "shell.execute_reply": "2023-09-05T22:51:06.021494Z"
    },
    "papermill": {
     "duration": 0.072248,
     "end_time": "2023-09-05T22:51:06.025907",
     "exception": false,
     "start_time": "2023-09-05T22:51:05.953659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20/3352524088.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('string')\n",
      "/tmp/ipykernel_20/3352524088.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('category')\n",
      "/tmp/ipykernel_20/3352524088.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('float')\n",
      "/tmp/ipykernel_20/3352524088.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('string')\n",
      "/tmp/ipykernel_20/3352524088.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('category')\n",
      "/tmp/ipykernel_20/3352524088.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('float')\n",
      "/tmp/ipykernel_20/3352524088.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('string')\n",
      "/tmp/ipykernel_20/3352524088.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('category')\n",
      "/tmp/ipykernel_20/3352524088.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('string')\n",
      "/tmp/ipykernel_20/3352524088.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('category')\n",
      "/tmp/ipykernel_20/3352524088.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('string')\n",
      "/tmp/ipykernel_20/3352524088.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('category')\n",
      "/tmp/ipykernel_20/3352524088.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('string')\n",
      "/tmp/ipykernel_20/3352524088.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('category')\n",
      "/tmp/ipykernel_20/3352524088.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('float')\n",
      "/tmp/ipykernel_20/3352524088.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('string')\n",
      "/tmp/ipykernel_20/3352524088.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('category')\n",
      "/tmp/ipykernel_20/3352524088.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('float')\n",
      "/tmp/ipykernel_20/3352524088.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('string')\n",
      "/tmp/ipykernel_20/3352524088.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('category')\n",
      "/tmp/ipykernel_20/3352524088.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('string')\n",
      "/tmp/ipykernel_20/3352524088.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('category')\n",
      "/tmp/ipykernel_20/3352524088.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('string')\n",
      "/tmp/ipykernel_20/3352524088.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[name] = dataframe[name].astype('category')\n"
     ]
    }
   ],
   "source": [
    "feature_columns = []\n",
    "convert_columns(train_ds_pd, 'Transported', feature_columns)\n",
    "convert_columns(valid_ds_pd, 'Transported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "009e9c99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:51:06.047093Z",
     "iopub.status.busy": "2023-09-05T22:51:06.046445Z",
     "iopub.status.idle": "2023-09-05T22:51:06.054359Z",
     "shell.execute_reply": "2023-09-05T22:51:06.053434Z"
    },
    "papermill": {
     "duration": 0.021495,
     "end_time": "2023-09-05T22:51:06.056868",
     "exception": false,
     "start_time": "2023-09-05T22:51:06.035373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, target=None, shuffle=True, batch_size=32):\n",
    "    df = dataframe.copy()\n",
    "    if target != None:\n",
    "        labels = df.pop(target)\n",
    "    #df = {key: value for key, value in dataframe.items()}\n",
    "    \n",
    "    if target != None:\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "        \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1867eae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:51:06.077566Z",
     "iopub.status.busy": "2023-09-05T22:51:06.076875Z",
     "iopub.status.idle": "2023-09-05T22:51:06.267605Z",
     "shell.execute_reply": "2023-09-05T22:51:06.266094Z"
    },
    "papermill": {
     "duration": 0.204579,
     "end_time": "2023-09-05T22:51:06.270670",
     "exception": false,
     "start_time": "2023-09-05T22:51:06.066091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_nn = df_to_dataset(train_ds_pd, 'Transported')\n",
    "valid_ds_nn = df_to_dataset(valid_ds_pd, 'Transported', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "725767bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:51:06.292484Z",
     "iopub.status.busy": "2023-09-05T22:51:06.292046Z",
     "iopub.status.idle": "2023-09-05T22:51:06.301479Z",
     "shell.execute_reply": "2023-09-05T22:51:06.300212Z"
    },
    "papermill": {
     "duration": 0.023183,
     "end_time": "2023-09-05T22:51:06.304114",
     "exception": false,
     "start_time": "2023-09-05T22:51:06.280931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for the feature.\n",
    "  normalizer = layers.Normalization(axis=None)\n",
    "\n",
    "  # Prepare a Dataset that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  # Create a layer that turns strings into integer indices.\n",
    "  if dtype == 'string':\n",
    "    index = layers.StringLookup(max_tokens=max_tokens)\n",
    "  # Otherwise, create a layer that turns integer values into integer indices.\n",
    "  else:\n",
    "    index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "  # Prepare a `tf.data.Dataset` that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Encode the integer indices.\n",
    "  encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "  # Apply multi-hot encoding to the indices. The lambda function captures the\n",
    "  # layer, so you can use them, or include them in the Keras Functional model later.\n",
    "  return lambda feature: encoder(index(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70c8f929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:51:06.325155Z",
     "iopub.status.busy": "2023-09-05T22:51:06.324733Z",
     "iopub.status.idle": "2023-09-05T22:51:06.330919Z",
     "shell.execute_reply": "2023-09-05T22:51:06.329699Z"
    },
    "papermill": {
     "duration": 0.019859,
     "end_time": "2023-09-05T22:51:06.333446",
     "exception": false,
     "start_time": "2023-09-05T22:51:06.313587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_types = ['HomePlanet', 'Destination', 'Deck', 'Cabin_num', 'Side']\n",
    "num_types = [lab for lab in train_ds_pd.columns.values if lab not in cat_types and lab != 'Transported']\n",
    "all_inputs = []\n",
    "encoded_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37cf15eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:51:06.355144Z",
     "iopub.status.busy": "2023-09-05T22:51:06.354459Z",
     "iopub.status.idle": "2023-09-05T22:51:10.665847Z",
     "shell.execute_reply": "2023-09-05T22:51:10.664667Z"
    },
    "papermill": {
     "duration": 4.325824,
     "end_time": "2023-09-05T22:51:10.668742",
     "exception": false,
     "start_time": "2023-09-05T22:51:06.342918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for header in num_types:\n",
    "    numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "    normalization_layer = get_normalization_layer(header, train_ds_nn)\n",
    "    encoded_numeric_col = normalization_layer(numeric_col)\n",
    "    all_inputs.append(numeric_col)\n",
    "    encoded_features.append(encoded_numeric_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "913db178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:51:10.690250Z",
     "iopub.status.busy": "2023-09-05T22:51:10.689811Z",
     "iopub.status.idle": "2023-09-05T22:51:12.666781Z",
     "shell.execute_reply": "2023-09-05T22:51:12.665566Z"
    },
    "papermill": {
     "duration": 1.990823,
     "end_time": "2023-09-05T22:51:12.669503",
     "exception": false,
     "start_time": "2023-09-05T22:51:10.678680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for header in cat_types:\n",
    "    categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "    encoding_layer = get_category_encoding_layer(name=header,\n",
    "                                               dataset=train_ds_nn,\n",
    "                                               dtype='string',\n",
    "                                               max_tokens=5)\n",
    "    encoded_categorical_col = encoding_layer(categorical_col)\n",
    "    all_inputs.append(categorical_col)\n",
    "    encoded_features.append(encoded_categorical_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2ed837b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:51:12.690026Z",
     "iopub.status.busy": "2023-09-05T22:51:12.689629Z",
     "iopub.status.idle": "2023-09-05T22:51:38.396766Z",
     "shell.execute_reply": "2023-09-05T22:51:38.395710Z"
    },
    "papermill": {
     "duration": 25.720681,
     "end_time": "2023-09-05T22:51:38.399460",
     "exception": false,
     "start_time": "2023-09-05T22:51:12.678779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "217/217 [==============================] - 5s 8ms/step - loss: 46.1881 - accuracy: 0.5009 - val_loss: 7.4071 - val_accuracy: 0.5143\n",
      "Epoch 2/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 11.0187 - accuracy: 0.5009 - val_loss: 6.0296 - val_accuracy: 0.5143\n",
      "Epoch 3/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 8.3901 - accuracy: 0.5009 - val_loss: 4.3367 - val_accuracy: 0.5143\n",
      "Epoch 4/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 6.3586 - accuracy: 0.5009 - val_loss: 3.2804 - val_accuracy: 0.5143\n",
      "Epoch 5/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 4.2150 - accuracy: 0.5009 - val_loss: 2.5716 - val_accuracy: 0.5143\n",
      "Epoch 6/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 3.2095 - accuracy: 0.5009 - val_loss: 1.6970 - val_accuracy: 0.5143\n",
      "Epoch 7/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 2.0549 - accuracy: 0.5009 - val_loss: 1.3584 - val_accuracy: 0.5143\n",
      "Epoch 8/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 1.3648 - accuracy: 0.5009 - val_loss: 0.8636 - val_accuracy: 0.5143\n",
      "Epoch 9/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 1.0229 - accuracy: 0.5009 - val_loss: 0.8257 - val_accuracy: 0.5143\n",
      "Epoch 10/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.7614 - accuracy: 0.5009 - val_loss: 0.6115 - val_accuracy: 0.5143\n",
      "Epoch 11/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.5009 - val_loss: 0.5214 - val_accuracy: 0.5143\n",
      "Epoch 12/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.5230 - accuracy: 0.5009 - val_loss: 0.5498 - val_accuracy: 0.5143\n",
      "Epoch 13/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.5115 - accuracy: 0.5009 - val_loss: 0.4927 - val_accuracy: 0.5143\n",
      "Epoch 14/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.4983 - accuracy: 0.5009 - val_loss: 0.4771 - val_accuracy: 0.5143\n",
      "Epoch 15/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.4729 - accuracy: 0.5009 - val_loss: 0.5185 - val_accuracy: 0.5143\n",
      "Epoch 16/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.4713 - accuracy: 0.5009 - val_loss: 0.4534 - val_accuracy: 0.5143\n",
      "Epoch 17/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.4572 - accuracy: 0.5009 - val_loss: 0.4830 - val_accuracy: 0.5143\n",
      "Epoch 18/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.4649 - accuracy: 0.5009 - val_loss: 0.4647 - val_accuracy: 0.5143\n",
      "Epoch 19/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.4332 - accuracy: 0.5009 - val_loss: 0.4723 - val_accuracy: 0.5143\n",
      "Epoch 20/20\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.4435 - accuracy: 0.5009 - val_loss: 0.4764 - val_accuracy: 0.5143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7bd2183e1ab0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.DenseFeatures(feature_columns),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='softmax')\n",
    "])\n",
    "\n",
    "nn_model.compile(optimizer='Adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "nn_model.fit(train_ds_nn,\n",
    "             validation_data=valid_ds_nn,\n",
    "             epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c127bf",
   "metadata": {
    "papermill": {
     "duration": 0.0428,
     "end_time": "2023-09-05T22:51:38.486720",
     "exception": false,
     "start_time": "2023-09-05T22:51:38.443920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd1bbf8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:51:38.575085Z",
     "iopub.status.busy": "2023-09-05T22:51:38.574688Z",
     "iopub.status.idle": "2023-09-05T22:51:38.710292Z",
     "shell.execute_reply": "2023-09-05T22:51:38.708988Z"
    },
    "papermill": {
     "duration": 0.182842,
     "end_time": "2023-09-05T22:51:38.713277",
     "exception": false,
     "start_time": "2023-09-05T22:51:38.530435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\n",
    "df_test = df_test.drop(['PassengerId', 'Name'], axis=1)\n",
    "df_test[[\"Deck\", \"Cabin_num\", \"Side\"]] = df_test[\"Cabin\"].str.split(\"/\", expand=True)\n",
    "df_test = df_test.drop('Cabin', axis=1)\n",
    "df_test.fillna(0, inplace=True)\n",
    "df_test['VIP'] = df_test['VIP'].astype(int)\n",
    "df_test['CryoSleep'] = df_test['CryoSleep'].astype(int)\n",
    "df_test.head(5)\n",
    "df_test.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "#for name in df_test.columns.values:\n",
    "#    if ~is_numeric_dtype(df_test[name]):\n",
    "#        df_test[name] = df_test[name].astype('category').cat.codes\n",
    "\n",
    "convert_columns(df_test)\n",
    "test_ds_nn = df_to_dataset(df_test, shuffle=False)\n",
    "#df = {key: value for key, value in df_test.items()}\n",
    "#ds = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "#test_ds_nn = tf.data.Dataset.from_tensor_slices(dict(df_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c66e3be6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:51:38.797913Z",
     "iopub.status.busy": "2023-09-05T22:51:38.797504Z",
     "iopub.status.idle": "2023-09-05T22:51:39.868803Z",
     "shell.execute_reply": "2023-09-05T22:51:39.867738Z"
    },
    "papermill": {
     "duration": 1.117216,
     "end_time": "2023-09-05T22:51:39.871696",
     "exception": false,
     "start_time": "2023-09-05T22:51:38.754480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = nn_model.predict(test_ds_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "420c87b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T22:51:39.962866Z",
     "iopub.status.busy": "2023-09-05T22:51:39.961916Z",
     "iopub.status.idle": "2023-09-05T22:51:40.007724Z",
     "shell.execute_reply": "2023-09-05T22:51:40.006337Z"
    },
    "papermill": {
     "duration": 0.094822,
     "end_time": "2023-09-05T22:51:40.010384",
     "exception": false,
     "start_time": "2023-09-05T22:51:39.915562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [0]*len(preds)\n",
    "predictions = [1 for pred in preds if pred > 0.5]\n",
    "predictions"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.6.4"
 },
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 60.911891,
   "end_time": "2023-09-05T22:51:42.511308",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-05T22:50:41.599417",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
